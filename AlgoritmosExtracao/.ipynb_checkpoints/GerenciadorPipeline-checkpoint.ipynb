{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0126ca4-ccb9-4538-8088-082850f2bec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook extracaoOnibus.ipynb to notebook\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O notebook foi executado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/felipe/anaconda3/bin/jupyter-nbconvert\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/jupyter_core/application.py\", line 283, in launch_instance\n",
      "    super().launch_instance(argv=argv, **kwargs)\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/nbconvert/nbconvertapp.py\", line 412, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/nbconvert/nbconvertapp.py\", line 590, in convert_notebooks\n",
      "    self.convert_single_notebook(notebook_filename)\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/nbconvert/nbconvertapp.py\", line 556, in convert_single_notebook\n",
      "    output, resources = self.export_single_notebook(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/nbconvert/nbconvertapp.py\", line 479, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/nbconvert/exporters/exporter.py\", line 203, in from_filename\n",
      "    return self.from_file(f, resources=resources, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/nbconvert/exporters/exporter.py\", line 222, in from_file\n",
      "    return self.from_notebook_node(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/nbconvert/exporters/notebook.py\", line 36, in from_notebook_node\n",
      "    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/nbconvert/exporters/exporter.py\", line 156, in from_notebook_node\n",
      "    nb_copy, resources = self._preprocess(nb_copy, resources)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/nbconvert/exporters/exporter.py\", line 354, in _preprocess\n",
      "    nbc, resc = preprocessor(nbc, resc)  # type:ignore[operator]\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/nbconvert/preprocessors/base.py\", line 48, in __call__\n",
      "    return self.preprocess(nb, resources)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/nbconvert/preprocessors/execute.py\", line 102, in preprocess\n",
      "    self.preprocess_cell(cell, resources, index)\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/nbconvert/preprocessors/execute.py\", line 123, in preprocess_cell\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/jupyter_core/utils/__init__.py\", line 165, in wrapped\n",
      "    return loop.run_until_complete(inner)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/nbclient/client.py\", line 1058, in async_execute_cell\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \"/home/felipe/anaconda3/lib/python3.12/site-packages/nbclient/client.py\", line 914, in _check_raise_for_error\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "import geopy\n",
      "import requests\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "from geopy.distance import geodesic\n",
      "import logging\n",
      "import pytz\n",
      "import threading\n",
      "import time\n",
      "import sqlite3\n",
      "\n",
      "# Configuração de logging\n",
      "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
      "\n",
      "# Conectar ao arquivo .db (ele será criado se não existir)\n",
      "conn = sqlite3.connect('dados_onibus.db')\n",
      "cursor = conn.cursor()\n",
      "\n",
      "# Criar tabela (se ainda não existir)\n",
      "cursor.execute('''\n",
      "    CREATE TABLE IF NOT EXISTS onibus (\n",
      "        COD INTEGER,\n",
      "        REFRESH TEXT,\n",
      "        LAT_IN_TIME REAL,\n",
      "        LON_IN_TIME REAL,\n",
      "        CODIGOLINHA INTEGER,\n",
      "        ADAPT INTEGER,\n",
      "        TIPO_VEIC INTEGER,\n",
      "        TABELA TEXT,\n",
      "        SITUACAO TEXT,\n",
      "        SITUACAO2 TEXT,\n",
      "        SENT TEXT,\n",
      "        TCOUNT INTEGER,\n",
      "        SENTIDO_IN_TIME TEXT,\n",
      "        HORA TEXT,\n",
      "        FLAG_PROCES INTEGER\n",
      "    )\n",
      "''')\n",
      "conn.commit()\n",
      "\n",
      "# Função para buscar e processar os dados de uma linha de ônibus\n",
      "def buscar_e_processar_dados(linha):\n",
      "    url_base = f'https://transporteservico.urbs.curitiba.pr.gov.br/getVeiculos.php?linha={linha:03}&c=821f0'\n",
      "    try:\n",
      "        response = requests.get(url_base, timeout=10)\n",
      "        response.raise_for_status()\n",
      "\n",
      "        timezone_sp = pytz.timezone('America/Sao_Paulo')\n",
      "        hora_online = datetime.now(timezone_sp).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
      "        dados_json = response.json()\n",
      "\n",
      "        if not dados_json or not isinstance(dados_json, dict):\n",
      "            return None\n",
      "\n",
      "        codigos_onibus = [{\n",
      "            \"COD\": valor['COD'],\n",
      "            \"REFRESH\": valor['REFRESH'],\n",
      "            \"LAT_IN_TIME\": valor['LAT'],\n",
      "            \"LON_IN_TIME\": valor['LON'],\n",
      "            \"CODIGOLINHA\": int(valor['CODIGOLINHA']),\n",
      "            \"ADAPT\": int(valor['ADAPT']),\n",
      "            \"TIPO_VEIC\": int(valor['TIPO_VEIC']),\n",
      "            \"TABELA\": valor['TABELA'],\n",
      "            \"SITUACAO\": valor['SITUACAO'],\n",
      "            \"SITUACAO2\": valor['SITUACAO2'],\n",
      "            \"SENT\": valor['SENT'],\n",
      "            \"TCOUNT\": valor.get('TCOUNT', 0),\n",
      "            \"SENTIDO_IN_TIME\": valor['SENTIDO']\n",
      "        } for chave, valor in dados_json.items()]\n",
      "\n",
      "        df_codigos_onibus = pd.DataFrame(codigos_onibus)\n",
      "        if not df_codigos_onibus.empty:\n",
      "            df_codigos_onibus['HORA'] = hora_online\n",
      "        else:\n",
      "            logging.warning(\"Nenhum ônibus encontrado para a linha %s\", linha)\n",
      "            return None\n",
      "\n",
      "        return df_codigos_onibus\n",
      "\n",
      "    except requests.Timeout:\n",
      "        logging.error(\"Timeout: A conexão para a linha %s demorou mais de 10 segundos\", linha)\n",
      "        return None\n",
      "    except requests.RequestException as e:\n",
      "        logging.error(\"Erro na requisição para a linha %s: %s\", linha, e)\n",
      "        return None\n",
      "    except Exception as e:\n",
      "        logging.error(\"Erro ao processar a linha %s: %s\", linha, e)\n",
      "        return None\n",
      "\n",
      "# Função para processar cada linha e armazenar os resultados em uma lista\n",
      "def processar_linha(linha, df_linhas, result_list):\n",
      "    df_result = buscar_e_processar_dados(linha)\n",
      "    if df_result is not None:\n",
      "        result_list.append(df_result)\n",
      "\n",
      "# Função para gravar os dados no banco de dados SQLite\n",
      "def gravar_no_banco(df_result):\n",
      "    for index, row in df_result.iterrows():\n",
      "        cursor.execute('''\n",
      "            INSERT INTO onibus (COD, REFRESH, LAT_IN_TIME, LON_IN_TIME, CODIGOLINHA, ADAPT, TIPO_VEIC, TABELA, \n",
      "                                SITUACAO, SITUACAO2, SENT, TCOUNT, SENTIDO_IN_TIME, HORA, FLAG_PROCES) \n",
      "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
      "        ''', (\n",
      "            row['COD'], row['REFRESH'], row['LAT_IN_TIME'], row['LON_IN_TIME'], row['CODIGOLINHA'],\n",
      "            row['ADAPT'], row['TIPO_VEIC'], row['TABELA'], row['SITUACAO'], row['SITUACAO2'], \n",
      "            row['SENT'], row['TCOUNT'], row['SENTIDO_IN_TIME'], row['HORA'], 0\n",
      "        ))\n",
      "    conn.commit()\n",
      "        # Remover as linhas que foram inseridas no banco de dados do DataFrame global\n",
      "    global df_concatenado\n",
      "    df_concatenado = df_concatenado.drop(df_result.index).reset_index(drop=True)\n",
      "\n",
      "\n",
      "\n",
      "# Obter os códigos únicos das linhas\n",
      "linhas_unicas = df_linhas['COD'].unique()\n",
      "\n",
      "# DataFrame final para armazenar os resultados concatenados\n",
      "df_concatenado = pd.DataFrame()\n",
      "\n",
      "# Função para gerenciar threads e processamento de dados\n",
      "def processar_linhas_com_threads(linhas_unicas, df_linhas):\n",
      "    global df_concatenado\n",
      "    i = 0\n",
      "    while True:\n",
      "        result_list = []\n",
      "\n",
      "        # Criar e iniciar threads para cada linha\n",
      "        threads = [\n",
      "            threading.Thread(target=processar_linha, args=(linha, df_linhas, result_list)) \n",
      "            for linha in linhas_unicas\n",
      "        ]\n",
      "\n",
      "        for thread in threads:\n",
      "            thread.start()\n",
      "\n",
      "        for thread in threads:\n",
      "            thread.join()\n",
      "\n",
      "        # Concatenar resultados e gravar no banco de dados\n",
      "        if result_list:\n",
      "            df_concatenado = pd.concat([df_concatenado] + result_list, ignore_index=True)\n",
      "            gravar_no_banco(df_concatenado)\n",
      "\n",
      "        logging.info('Número atual da iteração: %s', i)\n",
      "        break\n",
      "        i += 1\n",
      "\n",
      "        # Espera 60 segundos antes da próxima iteração\n",
      "        time.sleep(60)\n",
      "\n",
      "# Inicializar o processamento com threads\n",
      "try:\n",
      "    processar_linhas_com_threads(linhas_unicas, df_linhas)\n",
      "finally:\n",
      "    # Fechar a conexão com o banco de dados ao final\n",
      "    conn.close()\n",
      "\n",
      "------------------\n",
      "\n",
      "\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 116\u001b[0m\n",
      "\u001b[1;32m    111\u001b[0m     df_concatenado \u001b[38;5;241m=\u001b[39m df_concatenado\u001b[38;5;241m.\u001b[39mdrop(df_result\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Obter os códigos únicos das linhas\u001b[39;00m\n",
      "\u001b[0;32m--> 116\u001b[0m linhas_unicas \u001b[38;5;241m=\u001b[39m df_linhas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOD\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n",
      "\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# DataFrame final para armazenar os resultados concatenados\u001b[39;00m\n",
      "\u001b[1;32m    119\u001b[0m df_concatenado \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_linhas' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Comando para converter e executar o Jupyter Notebook\n",
    "notebook_path = 'extracaoOnibus.ipynb'\n",
    "subprocess.run(['jupyter', 'nbconvert', '--to', 'notebook', '--execute', notebook_path, '--output', 'output_notebook.ipynb'])\n",
    "\n",
    "print(\"O notebook foi executado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e6a1f5-1ec9-4cd5-920d-df41a3a1d195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O arquivo extracaoOnibus.ipynb foi encontrado.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "notebook_path = 'extracaoOnibus.ipynb'\n",
    "if not os.path.isfile(notebook_path):\n",
    "    print(f\"O arquivo {notebook_path} não foi encontrado.\")\n",
    "else:\n",
    "    print(f\"O arquivo {notebook_path} foi encontrado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef10358-83c5-4530-a7a6-af2d9a5dd4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f47002-3b25-4daf-886c-7128346390d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
