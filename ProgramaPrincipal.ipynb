{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0f49cb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m geodesic\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\__init__.py:176\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tester\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m test\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# use the closest tagged version if possible\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_versions\n\u001b[0;32m    178\u001b[0m v \u001b[38;5;241m=\u001b[39m get_versions()\n\u001b[0;32m    179\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosest-tag\u001b[39m\u001b[38;5;124m\"\u001b[39m, v[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:941\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1039\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import geopy\n",
    "import psycopg2\n",
    "from psycopg2 import sql, Error\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "import logging\n",
    "import pytz\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "            \n",
    "def validate_and_convert_types(row):\n",
    "    try:\n",
    "        # Define the expected types for each column\n",
    "        expected_types = {\n",
    "            'COD': str,\n",
    "            'Parada_Mais_Proxima': str,\n",
    "            'REFRESH': pd.Timestamp,\n",
    "            'LAT_IN_TIME': float,\n",
    "            'LON_IN_TIME': float,\n",
    "            'CODIGOLINHA': int,\n",
    "            'TABELA': str,\n",
    "            'SITUACAO': str,\n",
    "            'SITUACAO2': str,\n",
    "            'SENT': str,\n",
    "            'SENTIDO_IN_TIME': str,\n",
    "            'Hora': pd.Timestamp,\n",
    "            'Distancia_ate_ponto': float,\n",
    "            'SEQ': int,\n",
    "            'SEQ_MAX': int\n",
    "\n",
    "        }\n",
    "        \n",
    "        # Convert each column to the expected type\n",
    "        for column, expected_type in expected_types.items():\n",
    "            if column in row:\n",
    "                if expected_type == pd.Timestamp:\n",
    "                    row[column] = pd.to_datetime(row[column], errors='coerce')\n",
    "                else:\n",
    "                    row[column] = expected_type(row[column])\n",
    "\n",
    "        return row\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro ao converter tipos de dados: {e}\")\n",
    "        return None\n",
    "\n",
    "def inserir_bd(df_bus):\n",
    "    conn = None\n",
    "    try:\n",
    "        df = df_bus.copy()\n",
    "        if df['Sentido'].iloc[0] == 'sentido oposto':\n",
    "            df['SEQ'] = df['NOVO_SEQ']\n",
    "\n",
    "        conn_string = \"host='localhost' dbname='Banco_IC' user='postgres' password='felipecs'\"\n",
    "        conn = psycopg2.connect(conn_string)\n",
    "        with conn.cursor() as cur:\n",
    "            for index, row in df.iterrows():\n",
    "                # Validate and convert types\n",
    "                row = validate_and_convert_types(row)\n",
    "                if row is None:\n",
    "                    continue  # Skip this row if type conversion failed\n",
    "\n",
    "                insert_query = sql.SQL(\"\"\"\n",
    "                INSERT INTO data_onibus_sentido (COD, Parada_Mais_Proxima, REFRESH, LAT_IN_TIME, LON_IN_TIME, CODIGOLINHA, TABELA, SITUACAO, SITUACAO2, SENT, SENTIDO_IN_TIME, Hora, Distancia_ate_ponto, SEQ, SEQ_MAX)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                \"\"\")\n",
    "                cur.execute(insert_query, (\n",
    "                    row['COD'], \n",
    "                    row['Parada_Mais_Proxima'], \n",
    "                    row['REFRESH'], \n",
    "                    row['LAT_IN_TIME'], \n",
    "                    row['LON_IN_TIME'], \n",
    "                    row['CODIGOLINHA'], \n",
    "                    row['TABELA'], \n",
    "                    row['SITUACAO'], \n",
    "                    row['SITUACAO2'], \n",
    "                    row['SENT'], \n",
    "                    row['SENTIDO_IN_TIME'], \n",
    "                    row['Hora'], \n",
    "                    row['Distancia_ate_ponto'], \n",
    "                    row['SEQ'], \n",
    "                    row['SEQ_MAX'], \n",
    "                ))\n",
    "\n",
    "            conn.commit()\n",
    "            print(\"Dados inseridos com sucesso.\")\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        logging.error(f\"Erro ao inserir dados: {error}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "def inverte_nome_ponto(df, df_paradas_linha_1, df_paradas_linha_2, df_paradas_linha_3):\n",
    "    df_onibus = df.copy()\n",
    "\n",
    "    if 'new_parada_Mais_Proxima' not in df.columns:\n",
    "        df_onibus['new_parada_Mais_Proxima'] = ''\n",
    "            \n",
    "    if df_onibus['CODIGOLINHA'].iloc[0] == df_paradas_linha_1['COD'].iloc[0]:\n",
    "        df1 = df_paradas_linha_1.copy()\n",
    "    elif df_onibus['CODIGOLINHA'].iloc[0] == df_paradas_linha_2['COD'].iloc[0]:\n",
    "        df1 = df_paradas_linha_2.copy()\n",
    "    elif df_onibus['CODIGOLINHA'].iloc[0] == df_paradas_linha_3['COD'].iloc[0]:\n",
    "        df1 = df_paradas_linha_3.copy()\n",
    "    else:\n",
    "        logging.error(\"Linha de Ã´nibus nÃ£o encontrada nas paradas fornecidas.\")\n",
    "        return None\n",
    "    \n",
    "    mapeamento = dict(zip(df1['SEQ'], df1['Nome']))\n",
    "    df_onibus['new_parada_Mais_Proxima'] = df_onibus['NOVO_SEQ'].apply(lambda x: mapeamento.get(x, 'Parada nÃ£o encontrada'))\n",
    "    \n",
    "    return df_onibus\n",
    "\n",
    "def inverte_sequencia(data_bus):\n",
    "    df = data_bus.copy()\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    if 'NOVO_SEQ' not in df.columns:\n",
    "        df['NOVO_SEQ'] = 0\n",
    "\n",
    "    def retorna_novo_seq(df, index):\n",
    "        maior_seq = df['SEQ_MAX'].iloc[0]\n",
    "        seq_atual = df['SEQ'].iloc[index]\n",
    "        novo_seq = (maior_seq - seq_atual) + 1\n",
    "        return novo_seq\n",
    "\n",
    "    for index in range(df.shape[0]):\n",
    "        if df['Sentido'].iloc[index] == 'sentido oposto':\n",
    "            df.at[index, 'NOVO_SEQ'] = retorna_novo_seq(df, index)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def mandar_bd(df, df_paradas_linha_1, df_paradas_linha_2, df_paradas_linha_3):\n",
    "    try:\n",
    "        df_copy = df.copy()\n",
    "        # Initialize FLAG_CICLO if it does not exist\n",
    "        if 'FLAG_CICLO' not in df_copy.columns:\n",
    "            df_copy['FLAG_CICLO'] = False  # Initialize as boolean\n",
    "            \n",
    "        if 'NOVO_SEQ' not in df_copy.columns:\n",
    "            df_copy['NOVO_SEQ'] = 0\n",
    "\n",
    "        # Set FLAG_CICLO for SEQ == SEQ_MAX\n",
    "        df_copy['FLAG_CICLO'] = df_copy['SEQ'] == df_copy['SEQ_MAX']\n",
    "        df_copy.loc[(df_copy['SEQ'] == df_copy['SEQ_MAX']) & (df_copy['Sentido'] == 'sentido certo'), 'FLAG_CICLO'] = True\n",
    "        df_copy.loc[(df_copy['SEQ'] == 1) & (df_copy['Sentido'] == 'sentido oposto'), 'FLAG_CICLO'] = True\n",
    "        \n",
    "        # Remove duplicates based on specific columns and reset index\n",
    "        df = df_copy.drop_duplicates(subset=['COD', 'Parada_Mais_Proxima', 'Distancia_ate_ponto']).reset_index(drop=True)\n",
    "        \n",
    "        # List to store codes for removal\n",
    "        codigos_para_remover = []\n",
    "\n",
    "        # Iterate through the DataFrame\n",
    "        for index in range(len(df)):\n",
    "            cod_in = df['COD'].iloc[index]\n",
    "            flag = df['FLAG_CICLO'].iloc[index]\n",
    " \n",
    "            if flag:\n",
    "                cod_onibus_ciclo_fechado = cod_in\n",
    "                df_bd = df[df['COD'] == cod_onibus_ciclo_fechado].copy()\n",
    "                \n",
    "                if df_bd['Sentido'].iloc[0] == 'sentido oposto':\n",
    "                    df_sequencia_invertida = inverte_sequencia(df_bd)\n",
    "                    if df_sequencia_invertida is None:\n",
    "                        logging.error(\"Erro ao inverter ponto para o COD: %s\", cod_onibus_ciclo_fechado)\n",
    "                        continue\n",
    "      \n",
    "                    inserir_bd(df_sequencia_invertida)\n",
    "                elif df_bd['Sentido'].iloc[0] == 'sentido certo':\n",
    "\n",
    "                    inserir_bd(df_bd)\n",
    "                    \n",
    "                # Mark the code for removal\n",
    "                codigos_para_remover.append(cod_onibus_ciclo_fechado)\n",
    "\n",
    "        # Remove processed codes from DataFrame\n",
    "        df = df[~df['COD'].isin(codigos_para_remover)]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(\"Erro no processamento em mandar_bd: %s\", e)\n",
    "        return None\n",
    "\n",
    "def retorna_df_pontos_linha(linha):\n",
    "    url_base = 'https://transporteservico.urbs.curitiba.pr.gov.br/getPontosLinha.php?linha={}&c=821f0'\n",
    "    url = url_base.format(linha)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        dados_json = response.json()\n",
    "        data = {\n",
    "           \n",
    "            'Nome': [item['NOME'] for item in dados_json],\n",
    "            'NÃºmero': [item['NUM'] for item in dados_json],\n",
    "            'Latitude': [float(item['LAT'].replace(',', '.')) for item in dados_json],\n",
    "            'Longitude': [float(item['LON'].replace(',', '.')) for item in dados_json],\n",
    "            'SEQ': [int(item['SEQ']) for item in dados_json],\n",
    "            'GRUPO': [item['GRUPO'] for item in dados_json],\n",
    "            'Sentido': [item['SENTIDO'] for item in dados_json],\n",
    "            'Tipo': [item['TIPO'] for item in dados_json],\n",
    "            'ID do ItinerÃ¡rio': [int(item['ITINERARY_ID']) for item in dados_json]\n",
    "        }\n",
    "        df1 = pd.DataFrame(data)\n",
    "        df1['COD'] = linha\n",
    "        sentido_a = df1['Sentido'].iloc[0]\n",
    "        df_sentido_a = df1[df1['Sentido'] == sentido_a]\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(\"Falha ao acessar a pÃ¡gina: %s\", e)\n",
    "        return None, None\n",
    "\n",
    "\n",
    "\n",
    "    return df_sentido_a\n",
    "\n",
    "def calcular_distancia_ponto(df, df_pontos):\n",
    "    try:\n",
    "        distancias = []\n",
    "        for index, row in df.iterrows():\n",
    "            lat_onibus = row['LAT_IN_TIME']\n",
    "            lon_onibus = row['LON_IN_TIME']\n",
    "            ponto_mais_proximo = row['Parada_Mais_Proxima']\n",
    "            ponto = df_pontos[df_pontos['Nome'] == ponto_mais_proximo].iloc[0]\n",
    "            distancia = calcular_distancia(lat_onibus, lon_onibus, ponto['Latitude'], ponto['Longitude'])\n",
    "            distancias.append(distancia)\n",
    "        return distancias\n",
    "    except Exception as e:\n",
    "        logging.error(\"Erro ao calcular distÃ¢ncia atÃ© o ponto mais prÃ³ximo: %s\", e)\n",
    "        return None\n",
    "\n",
    "def calcular_distancia(lat1, lon1, lat2, lon2):\n",
    "    try:\n",
    "        coords_1 = (lat1, lon1)\n",
    "        coords_2 = (lat2, lon2)\n",
    "        distancia = geopy.distance.geodesic(coords_1, coords_2).meters\n",
    "        return distancia\n",
    "    except Exception as e:\n",
    "        logging.error(\"Erro ao calcular distÃ¢ncia: %s\", e)\n",
    "        return None\n",
    "    \n",
    "def encontrar_parada_mais_proxima(lat, lon, df_paradas):\n",
    "    try:\n",
    "        if df_paradas.empty:\n",
    "            logging.warning(\"O DataFrame df_paradas estÃ¡ vazio.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "        distancias = df_paradas.apply(lambda row: calcular_distancia(lat, lon, row['Latitude'], row['Longitude']), axis=1)\n",
    "        index_parada_mais_proxima = distancias.idxmin()\n",
    "        nome_parada_mais_proxima = df_paradas.loc[index_parada_mais_proxima, 'Nome']\n",
    "\n",
    "        return nome_parada_mais_proxima\n",
    "    except KeyError as ke:\n",
    "        logging.error(\"Chave nÃ£o encontrada: %s\", ke)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(\"Erro ao encontrar parada mais prÃ³xima: %s\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def manter_menor_distancia(df):\n",
    "    try:\n",
    "        # Aplica a funcao nsmallest para obter a menor distÃ¢ncia em cada grupo\n",
    "        df_menor_distancia = df.groupby(['COD', 'Parada_Mais_Proxima']).apply(lambda x: x.nsmallest(1, 'Distancia_ate_ponto')).reset_index(drop=True)\n",
    "        return df_menor_distancia\n",
    "    except Exception as e:\n",
    "        logging.error(\"Erro ao manter menor distÃ¢ncia: %s\", e)\n",
    "        return df\n",
    "\n",
    "\n",
    "def retorna_sentido(df):\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy.sort_values(by=['COD', 'Hora'], inplace=True)\n",
    "\n",
    "    # Adiciona a coluna 'Sentido' se naoexistir\n",
    "    if 'Sentido' not in df_copy.columns:\n",
    "        df_copy['Sentido'] = ''\n",
    "\n",
    "    # Obtem os cÃ³digos Ãºnicos\n",
    "    codigos_unicos = df_copy['COD'].unique()\n",
    "\n",
    "    # Itera sobre cada codigo unico\n",
    "    for cod in codigos_unicos:\n",
    "        df_cod = df_copy[df_copy['COD'] == cod].reset_index(drop=True)\n",
    "\n",
    "        # Verifica o sentido da sequencia\n",
    "        sentido = ''\n",
    "        for i in range(len(df_cod) - 1):\n",
    "            if df_cod['SEQ'].iloc[i] < df_cod['SEQ'].iloc[i + 1]:\n",
    "                sentido = 'sentido certo'\n",
    "            elif df_cod['SEQ'].iloc[i] > df_cod['SEQ'].iloc[i + 1]:\n",
    "                sentido = 'sentido oposto'\n",
    "            else:\n",
    "                sentido = 'onibus fora de tabela'\n",
    "\n",
    "\n",
    "        # Aplica o sentido calculado\n",
    "        df_copy.loc[df_copy['COD'] == cod, 'Sentido'] = sentido\n",
    "\n",
    "    return df_copy\n",
    "    \n",
    "def calcular_sentido_certo(df):\n",
    "    \n",
    "    df_codigos_onibus_sentido = df.copy()\n",
    "    for i in range(len(df_codigos_onibus_sentido)):\n",
    "            if(df_codigos_onibus_sentido['Sentido'].iloc[i] == 'sentido oposto'):\n",
    "                df_codigos_onibus_sentido['SEQ'].iloc[i] =(df_codigos_onibus_sentido['SEQ_MAX'].iloc[i] - df_codigos_onibus_sentido['SEQ'].iloc[i]) + 1\n",
    "    \n",
    "    return df_codigos_onibus_sentido\n",
    "                \n",
    "def retorna_seq_max(df, df_paradas):\n",
    "    # Copia o dataframe para evitar mudanÃ§as no original\n",
    "    df = df.copy()\n",
    "\n",
    "    # Converte a coluna 'CODIGOLINHA' para int\n",
    "    df['CODIGOLINHA'] = df['CODIGOLINHA'].astype(int)\n",
    "\n",
    "    # Adiciona a coluna 'SEQ_MAX' se nÃ£o existir\n",
    "    if 'SEQ_MAX' not in df.columns:\n",
    "        df['SEQ_MAX'] = 0\n",
    "\n",
    "    # Cria um dicionario de mapeamento de COD para o maior valor de SEQ\n",
    "    mapeamento_seq_max = df_paradas.groupby('COD')['SEQ'].max().to_dict()\n",
    "\n",
    "    # Aplica o mapeamento para obter SEQ_MAX\n",
    "    df['SEQ_MAX'] = df['CODIGOLINHA'].apply(lambda x: mapeamento_seq_max.get(x, 0))\n",
    "\n",
    "    return df\n",
    "\n",
    "                \n",
    "def buscar_e_processar_dados(linha, df_paradas_linha_1, df_paradas_linha_2, df_paradas_linha_3):\n",
    "\n",
    "    url_base = 'https://transporteservico.urbs.curitiba.pr.gov.br/getVeiculos.php?linha={}&c=821f0'.format(linha)\n",
    "    try:\n",
    "       \n",
    "        response = requests.get(url_base)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        timezone_sp = pytz.timezone('America/Sao_Paulo')\n",
    "        hora_online = datetime.now(timezone_sp).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        dados_json = response.json()\n",
    "        codigos_onibus = [{\n",
    "            \"COD\": valor['COD'],\n",
    "            \"REFRESH\": valor['REFRESH'],\n",
    "            \"LAT_IN_TIME\": valor['LAT'],\n",
    "            \"LON_IN_TIME\": valor['LON'],\n",
    "            \"CODIGOLINHA\": valor['CODIGOLINHA'],\n",
    "            \"TABELA\": valor['TABELA'],\n",
    "            \"SITUACAO\": valor['SITUACAO'],\n",
    "            \"SITUACAO2\": valor['SITUACAO2'],\n",
    "            \"SENT\": valor['SENT'],\n",
    "            \"SENTIDO_IN_TIME\": valor['SENTIDO']\n",
    "        } for chave, valor in dados_json.items()]\n",
    "\n",
    "        df_codigos_onibus = pd.DataFrame(codigos_onibus)\n",
    "        \n",
    "        #remover linhas que nÃ£o estÃ£o em operaÃ§Ã£o, mas aparecem no site\n",
    "        indices_para_remover = df_codigos_onibus[df_codigos_onibus['SITUACAO2'] != 'REALIZANDO ROTA'].index\n",
    "        # Removendo as linhas pelos Ã­ndices\n",
    "        \n",
    "        df_codigos_onibus.drop(index=indices_para_remover, inplace=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        df_codigos_onibus.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        if(df_paradas_linha_1['COD'].iloc[0] == linha):\n",
    "            \n",
    "            df_sentido_pontos = df_paradas_linha_1.copy()\n",
    "\n",
    "            \n",
    "        elif(df_paradas_linha_2['COD'].iloc[0] == linha):\n",
    "            \n",
    "            df_sentido_pontos = df_paradas_linha_2.copy()\n",
    "\n",
    "\n",
    "            \n",
    "        elif(df_paradas_linha_3['COD'].iloc[0] == linha):\n",
    "            \n",
    "            df_sentido_pontos = df_paradas_linha_3.copy()\n",
    "\n",
    "            \n",
    "        else:\n",
    "            logging.info(\"ERRO, DATFRAME DA LINHA NÃO INSERIDO\")\n",
    "            \n",
    "\n",
    "        df_codigos_onibus['Hora'] = hora_online\n",
    "        mapeamento = dict(zip(df_sentido_pontos['Nome'], df_sentido_pontos['SEQ']))\n",
    "    \n",
    "\n",
    "        # Encontrar parada mais proxima vetorizado\n",
    "        df_codigos_onibus['Parada_Mais_Proxima'] = df_codigos_onibus.apply(\n",
    "            lambda row: encontrar_parada_mais_proxima(row['LAT_IN_TIME'], row['LON_IN_TIME'], df_sentido_pontos),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        \n",
    "        df_codigos_onibus['Distancia_ate_ponto'] = calcular_distancia_ponto(df_codigos_onibus, df_sentido_pontos)\n",
    "        \n",
    "        df_codigos_onibus['SEQ'] = df_codigos_onibus['Parada_Mais_Proxima'].apply(lambda x: mapeamento.get(x))\n",
    "\n",
    "        df_codigos_onibus_max = retorna_seq_max(df_codigos_onibus,df_sentido_pontos)\n",
    "        \n",
    "\n",
    "       \n",
    "        return df_codigos_onibus_max\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        logging.error(\"Falha ao acessar a pÃ¡gina: %s\", e)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(\"Erro ao processar dados: %s\", e)\n",
    "        return None\n",
    "    \n",
    "\n",
    "    \n",
    "################################################################################################################################\n",
    "\n",
    "linhas = [203, 503, 303]\n",
    "\n",
    "df_paradas_linha_1 = retorna_df_pontos_linha(203)\n",
    "df_paradas_linha_2 = retorna_df_pontos_linha(503)\n",
    "df_paradas_linha_3 = retorna_df_pontos_linha(303)\n",
    "\n",
    "df_concatenado = pd.DataFrame()\n",
    "\n",
    "def processar_linha(linha, result_list):\n",
    "    try:\n",
    "        df_result = buscar_e_processar_dados(linha, df_paradas_linha_1, df_paradas_linha_2, df_paradas_linha_3)\n",
    "        if df_result is not None and not df_result.empty:\n",
    "            result_list.append(df_result)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro ao processar linha {linha}: {e}\")\n",
    "\n",
    "def dentro_do_horario():\n",
    "    agora = datetime.now().time()\n",
    "    inicio = datetime.strptime(\"05:30\", \"%H:%M\").time()\n",
    "    fim = datetime.strptime(\"23:30\", \"%H:%M\").time()\n",
    "    return inicio <= agora <= fim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loop principal\n",
    "while True:\n",
    "    if dentro_do_horario():\n",
    "        result_list = []\n",
    "        threads = [threading.Thread(target=processar_linha, args=(linha, result_list)) for linha in linhas]\n",
    "\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "        if result_list:\n",
    "            for df_result in result_list:\n",
    "                df_concatenado = pd.concat([df_concatenado, df_result], ignore_index=True)\n",
    "            df_concatenado = df_concatenado.drop_duplicates(['COD', 'REFRESH']).reset_index(drop=True)\n",
    "            df_concatenado = manter_menor_distancia(df_concatenado)\n",
    "            df_concatenado = retorna_sentido(df_concatenado)\n",
    "            df_concatenado = mandar_bd(df_concatenado, df_paradas_linha_1, df_paradas_linha_2, df_paradas_linha_3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Espera 60 segundos antes da próxima iteração\n",
    "        time.sleep(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
